{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Tiny area on the internet for everything about me. You can reach out to me at zeeshan060801 at gmail","title":"Home"},{"location":"about/","text":"About I am undergraduate researcher at International Institute of Information Technology (IIIT), Hyderabad. My interests lie in Quantum Computing and Computational Complexity. Currently (2024) I am in my fifth year of a dual-degree program. I have a tiny blog Devil in Detail that has my writings. I love teaching concepts that I find interesting, so you can also find some slides about the topics I have previously taught.","title":"About"},{"location":"about/#about","text":"I am undergraduate researcher at International Institute of Information Technology (IIIT), Hyderabad. My interests lie in Quantum Computing and Computational Complexity. Currently (2024) I am in my fifth year of a dual-degree program. I have a tiny blog Devil in Detail that has my writings. I love teaching concepts that I find interesting, so you can also find some slides about the topics I have previously taught.","title":"About"},{"location":"devil-in-details/","text":"Devil in Details When I was learning computational complexity, it was quite easy for me to convince myself that I understand the proofs even when there were cases when I would not. Going down the line, I would have troubles solving any problem, and I would get results that would make no sense. One of my friend then mentioned that \"The devil is always in the details.\" I would like to avoid that mistake, I dedicate this blog to a general theme of avoiding such superficial knowledge and look into the details. Writing about such details would be interesting, but it would be only possible consistently in the field I primarily focus on. The blog would still have writings from other random topics I have explored, academic or not. I have just (14.03.24) started to write things that I don't mind making public without polishing, there might not be a lot of entries on this page when you see it. Blog Entries On Cicero (14.03.24) Entry after I finished a book CAPTCHA and Quantum Supremacy (23.03.24) Connecting some dots about the nature of proving gaps between models of computation Zeroth Law of Thermodynamics (02.04.24) Simple discussion on the fundamental law LaTeX on mkdocs (03.04.24) Tutorial to get LaTeX running on mkdocs Polynomial Turing Machiens (03.04.24) Understanding the PSPACE complexity class","title":"Devil in Details"},{"location":"devil-in-details/#devil-in-details","text":"When I was learning computational complexity, it was quite easy for me to convince myself that I understand the proofs even when there were cases when I would not. Going down the line, I would have troubles solving any problem, and I would get results that would make no sense. One of my friend then mentioned that \"The devil is always in the details.\" I would like to avoid that mistake, I dedicate this blog to a general theme of avoiding such superficial knowledge and look into the details. Writing about such details would be interesting, but it would be only possible consistently in the field I primarily focus on. The blog would still have writings from other random topics I have explored, academic or not. I have just (14.03.24) started to write things that I don't mind making public without polishing, there might not be a lot of entries on this page when you see it.","title":"Devil in Details"},{"location":"devil-in-details/#blog-entries","text":"On Cicero (14.03.24) Entry after I finished a book CAPTCHA and Quantum Supremacy (23.03.24) Connecting some dots about the nature of proving gaps between models of computation Zeroth Law of Thermodynamics (02.04.24) Simple discussion on the fundamental law LaTeX on mkdocs (03.04.24) Tutorial to get LaTeX running on mkdocs Polynomial Turing Machiens (03.04.24) Understanding the PSPACE complexity class","title":"Blog Entries"},{"location":"devil-in-details/captcha/","text":"CAPTCHA and Quantum Supremacy I would like to run a very secure and safe city, I want everyone to quarantine for two days before I let them in. How do I manage to do this at all costs, without relying on a centralized authority and ensuring this happens even at the cost of all forms of collusion with the security? The only law then I can rely on are the laws of nature, ones that are impossible to break but easy to verify. I devise a tunnel as the only entrance to my city. These tunnels fit a person, it can fit no vehicles, no contraptions that would allow them to go faster. If the tunnels are long enough to take a healthy person atleast two days to walk , it'll ensure that a person has gone through a quarantine of two days before they could enter my city. I hadn't realised, but this is similar to the idea of Proof of Work that is being employed in popular blockchain ledgers, such as Bitcoin ledger. Rather than walking for two days to reach the city, we talk about performing computation to add transactions to the ledger. The computation is devised in a way that brute-force calculation is the only way to do so. The computation is performed by miners that are rewarded for their computation only if they find a solution, else their computation is gone to waste. To add a set of transactions to the ledger, the miner needs to calculate billions of hash values and find one that starts with x number of 0s. The number x is adjusted so that the total computation happening across the world combined to find it takes approximately 10 minutes. This is a simple way to ensure that set of transactions are added every 10 minutes. What happens when computation power increases? We increase x. In analogy, it is similar to increasing the length of the tunnel when the average walking speed has increased. Walking back to tunnels, what if I want more from the mechanism? Let's say there is an outbreak of this hypothetical disease that does not allow people to climb too many stairs at once, when an infected person tries to do so they lose their ability to walk for a few days, let's call it \"The Climber's Knee.\" This disease activates within one day and takes longer than two days to fade. It is very absurd and wonky, but bear with me. I want to improve my tunnels to ensure that only a healthy, i.e. a non-infected person is able to make it. If I put a bunch of stairs at the end of tunnel, then I would be certain that this person has been in quarantine for two days, and has not been infected when he reaches the top. I have now made my mechanism stronger, not only it ensures that a person goes through quarantine, but also they make out of it only if they are healthy. Climbing stairs is an easy task for the healthy, impossible for the infected. This core mechanism is used to tell apart entities across a lot of different domains. It's a very simple and generic idea but I find it very powerful. The Perfect CAPTCHA CAPTCHA's are different from traditional cryptography, they are more ad-hoc and not based on any particular laws. Their reliability is often due to the closed-sourceness. We define a perfect CAPTCHA by a task which is easy to certify, impossible for bots to solve but a mere chore for humans. The nature of this problem make it impossible to have a tunnel-like mechanism due to a blur-line that splits bots and users. We shift to an easier setting of the problem, what if there is a way of climbing the stairs at the end of tunnel by paying someone to carry you but it is quite expensive, and you'd rather just wait out till you are okay. This problem is no longer bounded by physical laws, but of financial ones. We separate the ones that are infected, as well as poor from the rest. Brutal, but this analogy makes sense in the CAPTCHA setting. We distinguish a human from a bot that has limited computing time and power, i.e. finance limitation. This problem is still relevant, because beyond a certain cost, it would be cheaper to hire a human to break it for you with ease rather than create a bot for it. As AI advances, the problems would have to get more creative to avoid the pitfalls of being solved by strong image models that are well capable and do not consume too much compute resource. This field is vast, and we leave the discussion of CAPTCHA at this state to move on to something more tangible. Achievable Quantum Supremacy We have talked about the physical impossibility, and we defined the notion of financial impossibility for given hardware status. When we are talking about CAPTCHA, it is difficult to measure this mathematically, or provide fundamental guarantees. That also explains the closed-source nature of their mechanism. Now we shift our problem to quantum supremacy, which can again be backed by physical laws. So far it has been difficult to find a use-case of quantum computing that have an edge over their classical counterparts. It is theoretically established that a quantum computer can do whatever a classical computer can, but we are yet to get a provable problem that a quantum computer can certainly do much faster than what our classical computers can. Finding such a case where the quantum computer is able to compute a solution which our current classical computers did not manage to would be quantum supremacy. But what we require is very different, theoretical limits of a classical computers are very far from what we can compute on our computers today. We require a solution that can be certified but not computable on the current state-of-the-art classical computers. This solution would be a computation that would be done on nature, a computation which humans can't artificially create, but one that requires computation impossible by current standards to even reverse-engineer the solution as one could do with factoring. Financial way of verification A non-theoretical way of verifying quantum supremacy would be to reverse the winning direction. Let's say we are talking about a task that is financially profitable, and by minimizing the resources required to perform this task would increase their profit margin. If a company claims that their quantum computer managed to solve this task, and it hasn't been solved by any classical computer we can trust their claim with high confidence. We do so because if the company had solved a task that no one else had classically, then it would be more beneficial for them to claim classical solution as it would be more feasible and profitable than running it on quantum computer. Such verification reduces the margin of advantage a quantum computer would have to show over the current classical computers, but would have to provide utility to the world. Why not factoring? Details Shor's algorithm has an exponential advantage over the current classical algorithms. One could think that this could be sufficient to prove quantum supremacy. Just try factorizing a number no classical computer can currently factor? The issue here is that it is difficult to verify whether the number picked to factor was not artificially chosen to be a product of known primes. If the party giving the number and factoring the number collude, this test would be easy to pass. Suppose a company claims quantum supremacy by factoring a number N=pq such that p and q are very large primes. It is not testifiable that the company did not pick these numbers before hand and multiplied them, multiplication is much easier than factoring. If the company claims they did not pick the number, but a separate party gave them, it is still easy to doubt, as there is very little guarantee that the parties did not decide N beforehand. Theoretically and practically, factoring is not the tunnel we are hunting for. The hunt for quantum supremacy is the hunt for stairs that current classical computers can't climb anytime soon, but a quantum computer was succcessful at. Sources Gotta CAPTCHA \u2019Em All link Bitcoin Whitepaper link Testing GPT on CAPTCHA link Scott Aaronson on Quantum Supremacy link Quantum computation and Shor's factoring algorithm link","title":"CAPTCHA and Quantum Supremacy"},{"location":"devil-in-details/captcha/#captcha-and-quantum-supremacy","text":"I would like to run a very secure and safe city, I want everyone to quarantine for two days before I let them in. How do I manage to do this at all costs, without relying on a centralized authority and ensuring this happens even at the cost of all forms of collusion with the security? The only law then I can rely on are the laws of nature, ones that are impossible to break but easy to verify. I devise a tunnel as the only entrance to my city. These tunnels fit a person, it can fit no vehicles, no contraptions that would allow them to go faster. If the tunnels are long enough to take a healthy person atleast two days to walk , it'll ensure that a person has gone through a quarantine of two days before they could enter my city. I hadn't realised, but this is similar to the idea of Proof of Work that is being employed in popular blockchain ledgers, such as Bitcoin ledger. Rather than walking for two days to reach the city, we talk about performing computation to add transactions to the ledger. The computation is devised in a way that brute-force calculation is the only way to do so. The computation is performed by miners that are rewarded for their computation only if they find a solution, else their computation is gone to waste. To add a set of transactions to the ledger, the miner needs to calculate billions of hash values and find one that starts with x number of 0s. The number x is adjusted so that the total computation happening across the world combined to find it takes approximately 10 minutes. This is a simple way to ensure that set of transactions are added every 10 minutes. What happens when computation power increases? We increase x. In analogy, it is similar to increasing the length of the tunnel when the average walking speed has increased. Walking back to tunnels, what if I want more from the mechanism? Let's say there is an outbreak of this hypothetical disease that does not allow people to climb too many stairs at once, when an infected person tries to do so they lose their ability to walk for a few days, let's call it \"The Climber's Knee.\" This disease activates within one day and takes longer than two days to fade. It is very absurd and wonky, but bear with me. I want to improve my tunnels to ensure that only a healthy, i.e. a non-infected person is able to make it. If I put a bunch of stairs at the end of tunnel, then I would be certain that this person has been in quarantine for two days, and has not been infected when he reaches the top. I have now made my mechanism stronger, not only it ensures that a person goes through quarantine, but also they make out of it only if they are healthy. Climbing stairs is an easy task for the healthy, impossible for the infected. This core mechanism is used to tell apart entities across a lot of different domains. It's a very simple and generic idea but I find it very powerful.","title":"CAPTCHA and Quantum Supremacy"},{"location":"devil-in-details/captcha/#the-perfect-captcha","text":"CAPTCHA's are different from traditional cryptography, they are more ad-hoc and not based on any particular laws. Their reliability is often due to the closed-sourceness. We define a perfect CAPTCHA by a task which is easy to certify, impossible for bots to solve but a mere chore for humans. The nature of this problem make it impossible to have a tunnel-like mechanism due to a blur-line that splits bots and users. We shift to an easier setting of the problem, what if there is a way of climbing the stairs at the end of tunnel by paying someone to carry you but it is quite expensive, and you'd rather just wait out till you are okay. This problem is no longer bounded by physical laws, but of financial ones. We separate the ones that are infected, as well as poor from the rest. Brutal, but this analogy makes sense in the CAPTCHA setting. We distinguish a human from a bot that has limited computing time and power, i.e. finance limitation. This problem is still relevant, because beyond a certain cost, it would be cheaper to hire a human to break it for you with ease rather than create a bot for it. As AI advances, the problems would have to get more creative to avoid the pitfalls of being solved by strong image models that are well capable and do not consume too much compute resource. This field is vast, and we leave the discussion of CAPTCHA at this state to move on to something more tangible.","title":"The Perfect CAPTCHA"},{"location":"devil-in-details/captcha/#achievable-quantum-supremacy","text":"We have talked about the physical impossibility, and we defined the notion of financial impossibility for given hardware status. When we are talking about CAPTCHA, it is difficult to measure this mathematically, or provide fundamental guarantees. That also explains the closed-source nature of their mechanism. Now we shift our problem to quantum supremacy, which can again be backed by physical laws. So far it has been difficult to find a use-case of quantum computing that have an edge over their classical counterparts. It is theoretically established that a quantum computer can do whatever a classical computer can, but we are yet to get a provable problem that a quantum computer can certainly do much faster than what our classical computers can. Finding such a case where the quantum computer is able to compute a solution which our current classical computers did not manage to would be quantum supremacy. But what we require is very different, theoretical limits of a classical computers are very far from what we can compute on our computers today. We require a solution that can be certified but not computable on the current state-of-the-art classical computers. This solution would be a computation that would be done on nature, a computation which humans can't artificially create, but one that requires computation impossible by current standards to even reverse-engineer the solution as one could do with factoring.","title":"Achievable Quantum Supremacy"},{"location":"devil-in-details/captcha/#financial-way-of-verification","text":"A non-theoretical way of verifying quantum supremacy would be to reverse the winning direction. Let's say we are talking about a task that is financially profitable, and by minimizing the resources required to perform this task would increase their profit margin. If a company claims that their quantum computer managed to solve this task, and it hasn't been solved by any classical computer we can trust their claim with high confidence. We do so because if the company had solved a task that no one else had classically, then it would be more beneficial for them to claim classical solution as it would be more feasible and profitable than running it on quantum computer. Such verification reduces the margin of advantage a quantum computer would have to show over the current classical computers, but would have to provide utility to the world.","title":"Financial way of verification"},{"location":"devil-in-details/captcha/#why-not-factoring-details","text":"Shor's algorithm has an exponential advantage over the current classical algorithms. One could think that this could be sufficient to prove quantum supremacy. Just try factorizing a number no classical computer can currently factor? The issue here is that it is difficult to verify whether the number picked to factor was not artificially chosen to be a product of known primes. If the party giving the number and factoring the number collude, this test would be easy to pass. Suppose a company claims quantum supremacy by factoring a number N=pq such that p and q are very large primes. It is not testifiable that the company did not pick these numbers before hand and multiplied them, multiplication is much easier than factoring. If the company claims they did not pick the number, but a separate party gave them, it is still easy to doubt, as there is very little guarantee that the parties did not decide N beforehand. Theoretically and practically, factoring is not the tunnel we are hunting for. The hunt for quantum supremacy is the hunt for stairs that current classical computers can't climb anytime soon, but a quantum computer was succcessful at.","title":"Why not factoring? Details"},{"location":"devil-in-details/captcha/#sources","text":"Gotta CAPTCHA \u2019Em All link Bitcoin Whitepaper link Testing GPT on CAPTCHA link Scott Aaronson on Quantum Supremacy link Quantum computation and Shor's factoring algorithm link","title":"Sources"},{"location":"devil-in-details/cicero/","text":"On Cicero Reading the biography of Marcus Tulius Cicero by Anthony Everitt felt like well written fiction full of content, suspense and appreciation towards the protagonist. Although it is unfair to the people of his time to consider him as the protagonist, his story puts him on the pedastal of no less than a hero for the Roman Republic, a failed one nonetheless due to circumstances. The story of Marcus Cicero came to a very depressing but a brave end, the last two years of his life after the death of Julius Ceasar makes him less of an untouchable character and more relatable due to his mistakes. The unstable and chaotic period of Cicero has given birth to many narratives and popular historical figures, and amongst them his narrative has managed to survive. This is partially luck, and majorly it is due to the influence he has had over the direction Roman republic steered. In a parallel universe, he would have joined the First Truimvirate and his narrative would have been twisted to match that of Caesar. Peering into his personal life via his letters, it made his character a vulnerable, insecure and a fame seeker who would still not give up on the fundamental believes of constitution and his duty towards the state. The fall of Roman Republic was best seen through a man who had previously saved and almost succeeded for the last time in his life. The perspectives helped me understand that unstable flow of the Republic in the last century of BCE, and how so many crucial moments could have shaped the history of Rome and by an extension the history of humanity. It is reductive to say that the series of events were inevitable, the turmoil could have lasted till the fall of Roman Republic completely, it could have skipped the Empire era. Looking back into the era, the most memorable moment for me remains his trail against Catalina, it was at the height of the political tension which in history could have been a complete hoax to protect Cicero's action. I personally view this as a artifact of historical manipulation and fragility that could sway the facts in favour of the writer. Marcus Tuluis Cicero extended his influence for the public good during his Consul, and this influence reached the same height after the death of Caesar when he applied his divide-and-conquer-lock-hands strategy to prevent the collusion of Antony and Octavious, and that makes him a legend of the last moments of the Roman Republic. Sources Cicero by Anthony Everitt goodreads","title":"On Cicero"},{"location":"devil-in-details/cicero/#on-cicero","text":"Reading the biography of Marcus Tulius Cicero by Anthony Everitt felt like well written fiction full of content, suspense and appreciation towards the protagonist. Although it is unfair to the people of his time to consider him as the protagonist, his story puts him on the pedastal of no less than a hero for the Roman Republic, a failed one nonetheless due to circumstances. The story of Marcus Cicero came to a very depressing but a brave end, the last two years of his life after the death of Julius Ceasar makes him less of an untouchable character and more relatable due to his mistakes. The unstable and chaotic period of Cicero has given birth to many narratives and popular historical figures, and amongst them his narrative has managed to survive. This is partially luck, and majorly it is due to the influence he has had over the direction Roman republic steered. In a parallel universe, he would have joined the First Truimvirate and his narrative would have been twisted to match that of Caesar. Peering into his personal life via his letters, it made his character a vulnerable, insecure and a fame seeker who would still not give up on the fundamental believes of constitution and his duty towards the state. The fall of Roman Republic was best seen through a man who had previously saved and almost succeeded for the last time in his life. The perspectives helped me understand that unstable flow of the Republic in the last century of BCE, and how so many crucial moments could have shaped the history of Rome and by an extension the history of humanity. It is reductive to say that the series of events were inevitable, the turmoil could have lasted till the fall of Roman Republic completely, it could have skipped the Empire era. Looking back into the era, the most memorable moment for me remains his trail against Catalina, it was at the height of the political tension which in history could have been a complete hoax to protect Cicero's action. I personally view this as a artifact of historical manipulation and fragility that could sway the facts in favour of the writer. Marcus Tuluis Cicero extended his influence for the public good during his Consul, and this influence reached the same height after the death of Caesar when he applied his divide-and-conquer-lock-hands strategy to prevent the collusion of Antony and Octavious, and that makes him a legend of the last moments of the Roman Republic.","title":"On Cicero"},{"location":"devil-in-details/cicero/#sources","text":"Cicero by Anthony Everitt goodreads","title":"Sources"},{"location":"devil-in-details/latex/","text":"LaTeX on mkdocs This is a brief instruction if you want to write LaTeX in your blog and if you use mkdocs to write them. Run the following command to install pymdown-extensions : pip install pymdown-extensions Once it is successfully installed, you need to add it to your config file, mkdocs.yaml . markdown_extensions: - pymdownx.arithmatex: generic: true extra_javascript: - javascripts/config.js - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js Close your server currently running and run it again. You should now be able to render LaTeX on your blog. Inline would be $\\pi <3$ : \\(\\pi <3\\) and large equations: $$ |\\psi(t)\\rangle = e^{-itH}|\\psi_0\\rangle $$ \\[ |\\psi(t)\\rangle = e^{-itH}|\\psi_0\\rangle \\] References arithmatex material","title":"LaTeX on mkdocs"},{"location":"devil-in-details/latex/#latex-on-mkdocs","text":"This is a brief instruction if you want to write LaTeX in your blog and if you use mkdocs to write them. Run the following command to install pymdown-extensions : pip install pymdown-extensions Once it is successfully installed, you need to add it to your config file, mkdocs.yaml . markdown_extensions: - pymdownx.arithmatex: generic: true extra_javascript: - javascripts/config.js - https://polyfill.io/v3/polyfill.min.js?features=es6 - https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js Close your server currently running and run it again. You should now be able to render LaTeX on your blog. Inline would be $\\pi <3$ : \\(\\pi <3\\) and large equations: $$ |\\psi(t)\\rangle = e^{-itH}|\\psi_0\\rangle $$ \\[ |\\psi(t)\\rangle = e^{-itH}|\\psi_0\\rangle \\]","title":"LaTeX on mkdocs"},{"location":"devil-in-details/latex/#references","text":"arithmatex material","title":"References"},{"location":"devil-in-details/pnpap/","text":"Polynomial Time Machines A Turing Machine can be determinisitic, non-deterministic or alternating. Under a polynomial time limitation, it is unclear whether any of these are a strict super-sets of the other. The classes are \\(\\text{P}, \\text{NP}, \\text{PH}\\) respectively. What's interesting is \\(\\text{PH} = \\text{PSPACE}\\) . The relation between these three classes is as follows: \\[ \\text{P} \\stackrel{\\subseteq}{\\tiny ?} \\text{NP} \\stackrel{\\subseteq}{\\tiny ?} \\text{PSPACE} \\] It is conjectured that all the subsets are strict. Cook-Levin Theorem required the locality of computation to produce the NP-completeness of SAT. Referring to Arora Barak's textbook on Complexity Theory, they guess that to make these subset equalities stricter, there is a need to use a new property of computation and/or the Turing Machine.","title":"Polynomial Time Machines"},{"location":"devil-in-details/pnpap/#polynomial-time-machines","text":"A Turing Machine can be determinisitic, non-deterministic or alternating. Under a polynomial time limitation, it is unclear whether any of these are a strict super-sets of the other. The classes are \\(\\text{P}, \\text{NP}, \\text{PH}\\) respectively. What's interesting is \\(\\text{PH} = \\text{PSPACE}\\) . The relation between these three classes is as follows: \\[ \\text{P} \\stackrel{\\subseteq}{\\tiny ?} \\text{NP} \\stackrel{\\subseteq}{\\tiny ?} \\text{PSPACE} \\] It is conjectured that all the subsets are strict. Cook-Levin Theorem required the locality of computation to produce the NP-completeness of SAT. Referring to Arora Barak's textbook on Complexity Theory, they guess that to make these subset equalities stricter, there is a need to use a new property of computation and/or the Turing Machine.","title":"Polynomial Time Machines"},{"location":"devil-in-details/pspace/","text":"Intuition on PSPACE class Languages decidable in finite time by a Deterministic Turing Machine with polynomial space all belong to the \\(\\text{PSPACE}\\) class. In this blog, we will describe the polynomial heirarchy in terms of Oracle access and show how it is equal to the PSPACE class. Rather than a textbook approach I would like to talk about it more intuitively and less formally. You can represent the size of the configuration of the TM in \\(\\mathcal{O}(S(n))\\) bits. If there is a path from the starting configuration to the accept configuration then the machine halts at accept. \\[ C_{acc} - C_{start} \\iff x \\in L \\] Grasping PSPACE There is not much to understand about the complexity class PSPACE on it's, unless it is compared to other classes. For example, PSPACE is certainly bigger than the class NP, because it can re-use the space to check all the non-deterministic paths of polynomial time. The same statement can be made about the relation between co-NP and PSPACE. Creating a Ladder \\(NP^{{NP^{NP}}^{\\dots}}\\) It is simple enough to compare NP, co-NP. One can go a few more steps ahead to make a ladder of NP oracles. \\(NP^{NP}\\) is a machine that can definitely solve problems from both NP and co-NP. The machine \\(NP^{NP}\\) has two-layer non-determinism. It can query for a non-deterministic solution, as well as use that solution in all of it's non-deterministic paths. Trivially it can solve NP problems by not choosing to use the oracle at all. Moreover it can solve co-NP as it can query the oracle for the solution, and flip the answer when necessary. Comparing the power of \\(NP^{NP}\\) to PSPACE, it is not difficult to see that PSPACE can simulate both the oracle as well as the non-deterministic machine in polynomial space each. Giving us \\(NP^{NP} \\subset PSPACE\\) . Regardless of how many ever polynomial-many oracles you stack up, they will remain within the simulation capability of the PSPACE class. Polynomial Heirarchy In the above section we described the ladder of NP oracles, this ladder is infact equivalent to another well-known set of classes called as the polynomial heirarchy. Polynomial heirarchy contains \\(\\sum_k^p\\) and \\(\\prod_k^p\\) . They defer by whether their base machine is an NP machine, or a co-NP machine. To represent the polynomial heirarchy in terms of a deterministic Turing Machine, we use the witness notation of NP and co-NP. \\[ L \\in \\text{NP} \\iff \\bigg( x \\in L \\iff \\exists v M(x, v) = 1 \\bigg) \\] Similarly, we define co-NP. \\[ L \\in \\text{co-NP} \\iff \\bigg( x \\in L \\iff \\forall v M(x, v) = 1 \\bigg) \\] Alternating Quantifiers To build one layer on top of NP, we can not just have two different existential quantifiers. Because these two sequential existential quantifiers can be absorbed into one \\(\\exists v_1, \\exists v_2 = \\exists v_3\\) where \\(v_3 = v_1\\cdot v_2\\) . Instead we need to alternate the quantifier from \\(\\exists\\) to \\(\\forall\\) . We define \\(\\sum_2^p\\) as follows: \\[ x \\in L \\iff \\exists v_1 \\forall v_2 M(x, v_1, v_2) = 1 \\] We show that \\(NP^{NP} = \\sum_2^p\\) . First we show \\(NP^{NP} \\subset \\sum_2^p\\) . The first existential quantifier takes care of the non-deterministic nature of NP machine. We then use the \\(\\forall\\) quantifier wherever the machine invokes the NP oracle. We flip the output incase the result deems to, accessing non-determinism in an oracular fashion allows the output flipping. Exercise: Show that \\(\\sum_2^p \\subset NP^{NP}\\) Once you solve the exercise, the equality is established. We define Truly Quantified Boolean Formula ( TQBF ) as the class union of any number of alternating quantifiers applied on a polynomial deterministic Turing Machine. \\[ \\text{TQBF} = \\cup_{k=1} \\sum_k^p \\] TQBF is PSPACE-complete The above section showed that the polynomial heirarchy can be represented by layering of NP oracles. This heirarchy itself lies withing PSPACE. You can convert this entire heirarchy to a single problem, which is TQBF. What we would like to show next is the equivalence between PSPACE and polynomial heirarchy. We refer back to the configuration space size as well as the condition for a string belonging to the language described earlier. Now we set a bound on the path length. Without loss of generality, we can claim that there are $2^{\\mathcal{O}(S(n))} nodes in the configuration space. The longest chain from \\(C_{start}\\) to \\(C_{accept}\\) .","title":"Intuition on PSPACE class"},{"location":"devil-in-details/pspace/#intuition-on-pspace-class","text":"Languages decidable in finite time by a Deterministic Turing Machine with polynomial space all belong to the \\(\\text{PSPACE}\\) class. In this blog, we will describe the polynomial heirarchy in terms of Oracle access and show how it is equal to the PSPACE class. Rather than a textbook approach I would like to talk about it more intuitively and less formally. You can represent the size of the configuration of the TM in \\(\\mathcal{O}(S(n))\\) bits. If there is a path from the starting configuration to the accept configuration then the machine halts at accept. \\[ C_{acc} - C_{start} \\iff x \\in L \\]","title":"Intuition on PSPACE class"},{"location":"devil-in-details/pspace/#grasping-pspace","text":"There is not much to understand about the complexity class PSPACE on it's, unless it is compared to other classes. For example, PSPACE is certainly bigger than the class NP, because it can re-use the space to check all the non-deterministic paths of polynomial time. The same statement can be made about the relation between co-NP and PSPACE.","title":"Grasping PSPACE"},{"location":"devil-in-details/pspace/#creating-a-ladder-npnpnpdots","text":"It is simple enough to compare NP, co-NP. One can go a few more steps ahead to make a ladder of NP oracles. \\(NP^{NP}\\) is a machine that can definitely solve problems from both NP and co-NP. The machine \\(NP^{NP}\\) has two-layer non-determinism. It can query for a non-deterministic solution, as well as use that solution in all of it's non-deterministic paths. Trivially it can solve NP problems by not choosing to use the oracle at all. Moreover it can solve co-NP as it can query the oracle for the solution, and flip the answer when necessary. Comparing the power of \\(NP^{NP}\\) to PSPACE, it is not difficult to see that PSPACE can simulate both the oracle as well as the non-deterministic machine in polynomial space each. Giving us \\(NP^{NP} \\subset PSPACE\\) . Regardless of how many ever polynomial-many oracles you stack up, they will remain within the simulation capability of the PSPACE class.","title":"Creating a Ladder \\(NP^{{NP^{NP}}^{\\dots}}\\)"},{"location":"devil-in-details/pspace/#polynomial-heirarchy","text":"In the above section we described the ladder of NP oracles, this ladder is infact equivalent to another well-known set of classes called as the polynomial heirarchy. Polynomial heirarchy contains \\(\\sum_k^p\\) and \\(\\prod_k^p\\) . They defer by whether their base machine is an NP machine, or a co-NP machine. To represent the polynomial heirarchy in terms of a deterministic Turing Machine, we use the witness notation of NP and co-NP. \\[ L \\in \\text{NP} \\iff \\bigg( x \\in L \\iff \\exists v M(x, v) = 1 \\bigg) \\] Similarly, we define co-NP. \\[ L \\in \\text{co-NP} \\iff \\bigg( x \\in L \\iff \\forall v M(x, v) = 1 \\bigg) \\]","title":"Polynomial Heirarchy"},{"location":"devil-in-details/pspace/#alternating-quantifiers","text":"To build one layer on top of NP, we can not just have two different existential quantifiers. Because these two sequential existential quantifiers can be absorbed into one \\(\\exists v_1, \\exists v_2 = \\exists v_3\\) where \\(v_3 = v_1\\cdot v_2\\) . Instead we need to alternate the quantifier from \\(\\exists\\) to \\(\\forall\\) . We define \\(\\sum_2^p\\) as follows: \\[ x \\in L \\iff \\exists v_1 \\forall v_2 M(x, v_1, v_2) = 1 \\] We show that \\(NP^{NP} = \\sum_2^p\\) . First we show \\(NP^{NP} \\subset \\sum_2^p\\) . The first existential quantifier takes care of the non-deterministic nature of NP machine. We then use the \\(\\forall\\) quantifier wherever the machine invokes the NP oracle. We flip the output incase the result deems to, accessing non-determinism in an oracular fashion allows the output flipping. Exercise: Show that \\(\\sum_2^p \\subset NP^{NP}\\) Once you solve the exercise, the equality is established. We define Truly Quantified Boolean Formula ( TQBF ) as the class union of any number of alternating quantifiers applied on a polynomial deterministic Turing Machine. \\[ \\text{TQBF} = \\cup_{k=1} \\sum_k^p \\]","title":"Alternating Quantifiers"},{"location":"devil-in-details/pspace/#tqbf-is-pspace-complete","text":"The above section showed that the polynomial heirarchy can be represented by layering of NP oracles. This heirarchy itself lies withing PSPACE. You can convert this entire heirarchy to a single problem, which is TQBF. What we would like to show next is the equivalence between PSPACE and polynomial heirarchy. We refer back to the configuration space size as well as the condition for a string belonging to the language described earlier. Now we set a bound on the path length. Without loss of generality, we can claim that there are $2^{\\mathcal{O}(S(n))} nodes in the configuration space. The longest chain from \\(C_{start}\\) to \\(C_{accept}\\) .","title":"TQBF is PSPACE-complete"},{"location":"devil-in-details/zeroth/","text":"Zeroth Law of Thermodynamics If two thermodynamic systems are both in thermal equilibrium with a third system, then the two systems are in thermal equilibrium with each other. The name had always intrigued me. The third law of thermodynamics was established in 1912. Decades later, Ralph Fowler established a law that was considered to be more fundamental than all the previous ones. The law can not be stated more simply, but it's inference leads to the description of Temperature . Consider a mercury thermometer, it in itself is a system that has defined properties. A thermometer is able to reach thermal equilibrium with the system whose temperature is to be measured without influencing the properties of the system itself. If this thermometer is able to reach thermal equilibrium with two separate systems, then by the zeroth law, the two systems are also in equilibrium with each other. Being in thermal equilibrium with each other, these systems have the same thermal temperature. I find this equivalence very cool, it establishes a metric based on just the relation of equivalence and nothing more. If we establish a direction of non-equilibrium, based on the direction of flow of heat, we define hot and cold . This addition of direction creates a heirarchy of temperature, based on the relation.","title":"Zeroth Law of Thermodynamics"},{"location":"devil-in-details/zeroth/#zeroth-law-of-thermodynamics","text":"If two thermodynamic systems are both in thermal equilibrium with a third system, then the two systems are in thermal equilibrium with each other. The name had always intrigued me. The third law of thermodynamics was established in 1912. Decades later, Ralph Fowler established a law that was considered to be more fundamental than all the previous ones. The law can not be stated more simply, but it's inference leads to the description of Temperature . Consider a mercury thermometer, it in itself is a system that has defined properties. A thermometer is able to reach thermal equilibrium with the system whose temperature is to be measured without influencing the properties of the system itself. If this thermometer is able to reach thermal equilibrium with two separate systems, then by the zeroth law, the two systems are also in equilibrium with each other. Being in thermal equilibrium with each other, these systems have the same thermal temperature. I find this equivalence very cool, it establishes a metric based on just the relation of equivalence and nothing more. If we establish a direction of non-equilibrium, based on the direction of flow of heat, we define hot and cold . This addition of direction creates a heirarchy of temperature, based on the relation.","title":"Zeroth Law of Thermodynamics"}]}